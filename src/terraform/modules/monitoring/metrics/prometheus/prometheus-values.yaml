# Override of : https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

kubeApiServer:
  enabled: true
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: true

prometheusOperator:
  admissionWebhooks:
    patch:
      podAnnotations:
        linkerd.io/inject: disabled
  # serviceMonitor:
  #   selfMonitor: false

# alertmanager:
#   serviceMonitor:
#     selfMonitor: false

prometheus:
  # serviceMonitor:
  #   selfMonitor: false
  prometheusSpec:
    scrapeInterval: "10s"
    scrapeTimeout: "10s"
    evaluationInterval: "10s"
    podMetadata:
      annotations:
        "linkerd.io/inject": "enabled"
    # resources:
    #   requests:
    #     #cpu: 250m
    #     memory: 200Mi
    #   limits:
    #     #cpu: 500m
    #     memory: 400Mi
    retention: 2h
    additionalScrapeConfigs:
#%{ if metrics_linkerd == true }
      - job_name: 'linkerd-controller'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - 'linkerd'
            - 'linkerd-viz'
        relabel_configs:
        - source_labels:
          - __meta_kubernetes_pod_container_port_name
          action: keep
          regex: admin-http
        - source_labels: [__meta_kubernetes_pod_container_name]
          action: replace
          target_label: component

      - job_name: 'linkerd-service-mirror'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels:
          - __meta_kubernetes_pod_label_linkerd_io_control_plane_component
          - __meta_kubernetes_pod_container_port_name
          action: keep
          regex: linkerd-service-mirror;admin-http$
        - source_labels: [__meta_kubernetes_pod_container_name]
          action: replace
          target_label: component

      - job_name: 'linkerd-proxy'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels:
          - __meta_kubernetes_pod_container_name
          - __meta_kubernetes_pod_container_port_name
          - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
          action: keep
          regex: ^linkerd-proxy;linkerd-admin;linkerd$
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: pod
        # special case k8s' "job" label, to not interfere with prometheus' "job"
        # label
        # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
        # k8s_job=foo
        - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job]
          action: replace
          target_label: k8s_job
        # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
        - action: labeldrop
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
        # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
        # deployment=foo
        - action: labelmap
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
        # drop all labels that we just made copies of in the previous labelmap
        - action: labeldrop
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
        # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
        # foo=bar
        - action: labelmap
          regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
        # Copy all pod labels to tmp labels
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
          replacement: __tmp_pod_label_$1
        # Take `linkerd_io_` prefixed labels and copy them without the prefix
        - action: labelmap
          regex: __tmp_pod_label_linkerd_io_(.+)
          replacement:  __tmp_pod_label_$1
        # Drop the `linkerd_io_` originals
        - action: labeldrop
          regex: __tmp_pod_label_linkerd_io_(.+)
        # Copy tmp labels into real labels
        - action: labelmap
          regex: __tmp_pod_label_(.+)
#%{ endif }
#%{ if metrics_minio == true }
      - job_name: minio-job
        metrics_path: /minio/v2/metrics/cluster
        scheme: http
        static_configs:
        - targets: ['minio.minio.svc.cluster.local:9000']
#%{ endif }
      - job_name: otel-collector
        static_configs:
        - targets: ['otel-collector-collector-monitoring.opentelemetry.svc.cluster.local:8888']
      - job_name: otel-metrics
        static_configs:
        - targets: ['otel-collector-collector.opentelemetry.svc.cluster.local:8889']
  ingress:
    enabled: true
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
      traefik.ingress.kubernetes.io/router.middlewares: default-redirect-http@kubernetescrd
    hosts:
      - prometheus.infrastructure.${domain-name}
    path: /
    pathType: Prefix
    tls:
      - secretName: prometheus-cert
        hosts:
        - prometheus.infrastructure.${domain-name}

grafana:
  # serviceMonitor:
  #   enabled: false
  adminPassword: ${grafana_password}
  defaultDashboardsEnabled: true
  sidecar:
    dashboards:
      searchNamespace: ALL
      enabled: true
      label: grafana_dashboard
      folder: /tmp/dashboards
      provider:
        foldersFromFilesStructure: true
      annotations:
        k8s-sidecar-target-directory: /tmp/dashboards/Infrastructure/Kubernetes
  ingress:
    enabled: true
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
      traefik.ingress.kubernetes.io/router.middlewares: default-redirect-http@kubernetescrd
    hosts:
      - grafana.infrastructure.${domain-name}
    path: /
    pathType: Prefix
    tls:
      - secretName: prometheus-cert
        hosts:
        - grafana.infrastructure.${domain-name}
#%{ if metrics_loki == true }
  additionalDataSources:
    - name: Loki
      access: proxy
      orgId: 1
      type: loki
      url: http://loki.loki.svc.cluster.local:3100
      version: 1
      editable: true
      uid: loki
#%{ if metrics_tempo == true }
      jsonData:
        derivedFields:
        - datasourceUid: tempo
          matcherRegex: '"traceid":"(\w+)"'
          name: 'Trace ID'
          url: '$$${__value.raw}'
          urlDisplayLabel: "Trace ID"
          internalLink: true
#%{ endif }
#%{ endif }
#%{ if metrics_tempo == true }
    - name: Tempo
      type: tempo
      access: proxy
      orgId: 1
      url: http://tempo.tempo.svc.cluster.local:3100
      basicAuth: false
      isDefault: false
      version: 1
      editable: true
      apiVersion: 1
      jsonData:
        tracesToLogs:
          datasourceUid: 'loki'
          tags: ['Trace ID', 'instance', 'pod', 'namespace', 'app']
          mappedTags: [{ key: 'service.name', value: 'app'}, { key: 'Trace ID', value: 'traceId' }, { key: 'Span ID', value: 'spanId' }]
          mapTagNamesEnabled: true
          spanStartTimeShift: '1h'
          spanEndTimeShift: '1h'
          filterByTraceID: true
          filterBySpanID: false
        serviceMap:
          datasourceUid: prometheus
      search:
        hide: false
      nodeGraph:
        enabled: true
      lokiSearch:
        datasourceUid: 'loki'
      uid: tempo
#%{ endif }
  plugins:
  - grafana-piechart-panel
  - digrich-bubblechart-panel
  - grafana-clock-panel

# prometheus-node-exporter:
#   prometheus:
#     monitor:
#       enabled: false

# kubeControllerManager:
#   serviceMonitor:
#     enabled: true

# coreDns:
#   serviceMonitor:
#     enabled: false

# kubeDns:
#   serviceMonitor:
#     enabled: false

# kubeScheduler:
#   serviceMonitor:
#     enabled: false

# kubeProxy:
#   serviceMonitor:
#     enabled: false

# kube-state-metrics:
#   prometheus:
#     serviceMonitor:
#       enabled: false

# kubelet:
#   serviceMonitor:
#     probes: false

# kubeEtcd:
#   serviceMonitor:
#     enabled: false
